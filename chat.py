import torch
import os
import codecs
from ouro import Ouro
from data_tools import RUNNING_CONFIG, ByteTokenizer 


def chat(model_path: str = "checkpoints/gridman_s_sft.pt", 
         device_str: str = "cuda", 
         temperature: float = 0.38, 
         top_k: int = 10, 
         max_new_bytes: int = 2048):
    # ç¯å¢ƒå‡†å¤‡
    config = RUNNING_CONFIG
    device = torch.device(device_str if torch.cuda.is_available() else "cpu")
    tokenizer = ByteTokenizer()
    
    # ç‰¹æ®Šå­—èŠ‚å®šä¹‰
    EOS_ID = config.tokenizer.eos_token_id
    PAD_ID = config.tokenizer.pad_token_id
    PATCH_SIZE = config.patch_size

    # æ¨¡å‹åŠ è½½
    print(f"âš¡ Gridman Chat Mode | Temp: {temperature} | TopK: {top_k} | Device: {device}")
    model = Ouro(config).to(device)
    
    if os.path.exists(model_path):
        print(f"ğŸ“‚ Loading weights from {model_path}...")
        checkpoint = torch.load(model_path, map_location=device)
        # å¤„ç†å¯èƒ½çš„ DDP åŒ…è£…å‰ç¼€
        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint
        new_state_dict = {k.replace("_orig_mod.", ""): v for k, v in state_dict.items()}
        model.load_state_dict(new_state_dict, strict=False)
    else:
        print("âš ï¸ Warning: No checkpoint found, using random weights.")

    model.eval()
    
    _states: tuple[torch.Tensor, ...] = checkpoint['states']
    states = tuple(s.to(device) for s in _states)

    print("\n" + "="*50)
    print("Gridman å¯åŠ¨å®Œæ¯•. è¾“å…¥ 'exit' é€€å‡º. ")
    print("="*50 + "\n")

    while True:
        try:
            user_input = input("User > ")
            if not user_input: continue
            if user_input.lower() in ['exit', 'quit']: break
        except EOFError:
            break
        
        print("Gridman > ", end="", flush=True)

        # å°†ç”¨æˆ·è¾“å…¥ç¼–ç ä¸ºå­—èŠ‚ ID
        input_ids = tokenizer.encode(user_input)
        total_len = len(input_ids)
        
        # å‡†å¤‡å¢é‡è§£ç å™¨å¤„ç† UTF-8 æµ
        decoder = codecs.getincrementaldecoder("utf-8")(errors='replace')
        
        # åˆå§‹åŒ–ä¸Šä¸€è½®çš„è¾“å‡º Patch ä¸ºå…¨ 0 (é™é»˜æ€) ä»¥å¯åŠ¨é€’å½’
        current_input_patch = torch.full((1, 1, PATCH_SIZE), PAD_ID, dtype=torch.long, device=device)
        last_real_byte = PAD_ID
        
        # å‰ç¼€æ³¨å…¥
        i = 0
        while i < total_len:
            chunk_len = min(PATCH_SIZE, total_len - i)
            user_chunk = input_ids[i : i + chunk_len]
            
            # æ„é€ å¼ºåˆ¶å‰ç¼€å’Œèµ·å§‹å­—èŠ‚
            prefix_tensor = torch.tensor([user_chunk], dtype=torch.long, device=device) # [1, L]
            sos_tensor = torch.tensor([[last_real_byte]], dtype=torch.long, device=device) # [1, 1]
            
            with torch.no_grad():
                _, logits, next_states, _, _ = model(
                    input_patches=current_input_patch, 
                    target_patches=None, 
                    states=states,
                    override_last_tokens=sos_tensor,
                    force_prefix=prefix_tensor,
                    temperature=temperature,
                    top_k=top_k
                )
                states = next_states
            
            # è·å–ç”Ÿæˆçš„ Patch
            next_patch_ids = torch.argmax(logits[:, 0, :, :], dim=-1) # [1, P]
            patch_list = next_patch_ids[0].cpu().tolist()
            
            # å¦‚æœè¿™æ˜¯ä¸€ä¸ªä¸å®Œæ•´çš„æœ«å°¾ Patch, æ‰“å°æ¨¡å‹è‡ªåŠ¨è¡¥å…¨çš„éƒ¨åˆ†
            if i + chunk_len >= total_len and chunk_len < PATCH_SIZE:
                generated_part = patch_list[chunk_len:]
                valid_bytes = bytes([b for b in generated_part if b < 256])
                print(decoder.decode(valid_bytes, final=False), end="", flush=True)

            # æ›´æ–°å¾ªç¯çŠ¶æ€
            current_input_patch = next_patch_ids.unsqueeze(1) # [1, 1, P]
            last_real_byte = patch_list[-1]
            i += chunk_len

        # è‡ªç”±ç”Ÿæˆ 
        generated_count = 0
        stop_generation = False
        
        while generated_count < max_new_bytes and not stop_generation:
            sos_tensor = torch.tensor([[last_real_byte]], dtype=torch.long, device=device)
            
            with torch.no_grad():
                _, logits, next_states, _, _ = model(
                    input_patches=current_input_patch, 
                    target_patches=None, 
                    states=states,
                    override_last_tokens=sos_tensor,
                    force_prefix=None,
                    temperature=temperature,
                    top_k=top_k
                )
                states = next_states

            # æå–ç”Ÿæˆçš„ token
            next_patch_ids = torch.argmax(logits[:, 0, :, :], dim=-1)
            patch_list = next_patch_ids[0].cpu().tolist()
            
            # æ£€æŸ¥ EOS
            output_patch = []
            for b in patch_list:
                if b == EOS_ID:
                    stop_generation = True
                    break
                if b < 256: # ä»…å¤„ç†æœ‰æ•ˆå­—èŠ‚
                    output_patch.append(b)
            
            # è§£ç å¹¶å®æ—¶æ‰“å°
            print(decoder.decode(bytes(output_patch), final=stop_generation), end="", flush=True)
            
            # æ›´æ–°çŠ¶æ€
            current_input_patch = next_patch_ids.unsqueeze(1)
            last_real_byte = patch_list[-1]
            generated_count += len(patch_list)
            
            if stop_generation:
                break
        
        print("") # æ¢è¡Œå¤„ç†ä¸‹ä¸€è½®å¯¹è¯


if __name__ == "__main__":
    chat()
