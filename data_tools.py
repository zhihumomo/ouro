import math
import os
from typing import Callable, Optional, Tuple
import torch
import torch.nn as nn

from torch.utils.data import DataLoader
from dataclasses import dataclass
from itertools import chain
from torch.utils.data import Dataset

from datasets import DatasetDict, Dataset as DS


SQRT2 = math.sqrt(2)


# å­—èŠ‚åˆ†è¯å™¨
class ByteTokenizer:
    """
    å­—èŠ‚ Tokenizer, æ— éœ€è®­ç»ƒ, å…¨ä¸–ç•Œé€šç”¨
    """
    def __init__(self):
        # å¡«å……ç¬¦
        self.pad_token_id = 256 
        # ç»ˆæ­¢ç¬¦
        self.eos_token_id = 257

    def encode(self, text: str) -> list[int]:
        # å°†æ–‡æœ¬è½¬ä¸º UTF-8 å­—èŠ‚åˆ—è¡¨
        return list(text.encode('utf-8'))

    def decode(self, ids: list[int]) -> str:
        # è¿‡æ»¤ç‰¹æ®Š token å¹¶è§£ç 
        clean_ids = [i for i in ids if 0 <= i < 256]
        return bytes(clean_ids).decode('utf-8', errors='replace')
    
    def __call__(self, text, **kwargs):
        return {"input_ids": self.encode(text)}
    

@dataclass
class OscillatorConfig:
    hidden_dim: int = 2         # 2ç»´ (Re, Im)
    frequency: float = 0.05     # è§’é€Ÿåº¦
    amplitude: float = 1.2      # æ³¢åŠ¨å¯¹ Logits çš„å½±å“å¼ºåº¦
    noise_std: float = 0.15     # çŠ¶æ€å™ªå£°
    coupling: float = 0.05      # è¾“å…¥ä¿¡å·å¯¹æŒ¯è¡å™¨çš„æ‰°åŠ¨å¼ºåº¦


@dataclass
class StateTransformerConfig:
    """
    StateTransformer é…ç½®ç±»
    """
    embed_dim: int
    layers: int
    heads: int
    dff: int

    # çŠ¶æ€ä¸è®°å¿†
    states_len: int = 0 
    mem_len: int = 0  

    # Actor ä¸“ç”¨
    num_anchors: int = 4
    use_cross_attn: bool = False   
   


class Config:
    """
    ä¸»æ¨¡å‹é…ç½®ç±»
    """
    def __init__(self, embed_dim: int, 
                 heads: int, sensor_layers: int, brain_layers: int, actor_layers: int,
                 wake_steps: int,
                 pretrain_steps: int,
                 sft_steps: int,
                 checkpoint_name: str,
                 data_base='path/datasets',
                 tokenizer=ByteTokenizer()
                 ) -> None:
        # Tokenizer
        self.tokenizer = tokenizer

        # æ¨¡å‹å‚æ•°
        self.embed_dim = embed_dim
        self.states_len = self.embed_dim // 2

        self.chunk_size = embed_dim
        self.patch_size = embed_dim // 8
        self.byte_embed_dim = 258 
        self.byte_vocab_size = 258                      

        self.heads = heads

        self.wake_steps = wake_steps
        self.sleep_steps = max(self.wake_steps // 2, 1)

        self.bptt_span = 4

        self.cycle_len = self.wake_steps + self.sleep_steps
        self.osc_freq = 2 * math.pi / self.cycle_len  

        self.max_ponder_steps = self.wake_steps + self.sleep_steps

        # ç¥ç»æŒ¯è¡å™¨
        self.oscillator_config = OscillatorConfig()

        # Sensor é…ç½®
        self.sensor_config = StateTransformerConfig(
            embed_dim=self.embed_dim,                # å¿…é¡»ä¸ Brian ä¸€è‡´æ‰èƒ½å…±äº« State
            layers=sensor_layers,                    # è½»é‡çº§, åšç®€å•çš„ä¸Šä¸‹æ–‡æ··åˆ
            heads=self.heads, 
            dff=self.embed_dim * 4,
            states_len=self.states_len,              # ä¸ Brian å…±äº«ç›¸åŒçš„çŠ¶æ€é•¿åº¦
            mem_len=self.states_len * 4              # ä¸ Brian å…±äº«ç›¸åŒçš„è®°å¿†é•¿åº¦
        )

        # Brain é…ç½®
        self.brain_config = StateTransformerConfig(
            embed_dim=self.embed_dim,                
            layers=brain_layers,                   
            heads=self.heads, 
            dff=self.embed_dim * 4,
            states_len=self.states_len,              
            mem_len=self.states_len * 4    
        )

        # Actor é…ç½®, Actor åªæ˜¯ç®€å•çš„ HormoneTransformer
        # è¿™é‡Œä¸ºäº†æ–¹ä¾¿ä¾ç„¶ä½¿ç”¨ StateTransformerConfig é…ç½®ç±»
        self.actor_config = StateTransformerConfig(
            embed_dim=self.embed_dim,                
            layers=actor_layers,                   
            heads=self.heads, 
            dff=self.embed_dim * 4,
            use_cross_attn=True,
            num_anchors=self.patch_size // 16
        )

        # è®­ç»ƒé…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.warmip_steps = 1000
        
        self.grad_clip_norm = 1.0
        self.seed = 42

        # æ•°æ®é›†è·¯å¾„
        self.data_base = data_base
        self.response_start_token = '\nGridman: '

        # é¢„è®­ç»ƒé…ç½®
        self.pretrain_train_file = f'{self.data_base}/pretrain_hq/pretrain_hq_train.jsonl'
        self.pretrain_val_file = f'{self.data_base}/pretrain_hq/pretrain_hq_val.jsonl'
        self.pretrain_steps = pretrain_steps 

        # SFT é…ç½®
        self.sft_train_file = f'{self.data_base}/sft_mini_512/sft_mini_512_train.jsonl'
        self.sft_val_file = f'{self.data_base}/sft_mini_512/sft_mini_512_val.jsonl'
        self.sft_steps = sft_steps
        
        # æ—¥å¿—é…ç½®
        self.logging_steps = 1000
        self.val_batches = 50     

        # ä¿å­˜é…ç½®
        self.save_steps = 5000 
        self.checkpoint_dir = './checkpoints'
        self.checkpoint_name = checkpoint_name

        # å­¦ä¹ ç‡é…ç½®
        self.base_lr = 3e-4
        self.sft_lr = 3e-6
        self.lr = self.lr_func

    def lr_func(self, update_count=0, is_sft=False):
        """
        è¿”å›æ ‡å‡†ä½™å¼¦é€€ç«(Cosine Annealing)ç­–ç•¥ä¸‹, å½“å‰ update_count å¯¹åº”çš„ç†è®ºå­¦ä¹ ç‡
        ç”¨äºåœ¨ Resume æ—¶æ ¡å‡†ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        """
        # ç¡®å®šè¯¥é˜¶æ®µçš„è¶…å‚æ•°
        if is_sft:
            max_lr = self.sft_lr
            # è®¡ç®— SFT é˜¶æ®µçš„æ€»æ›´æ–°æ¬¡æ•° (Total Updates)
            total_updates = self.sft_steps // self.bptt_span
        else:
            max_lr = self.base_lr
            # è®¡ç®— Pretrain é˜¶æ®µçš„æ€»æ›´æ–°æ¬¡æ•°
            total_updates = self.pretrain_steps // self.bptt_span
            
        # è®¾å®šæœ€å°å­¦ä¹ ç‡ 
        min_lr = max_lr * 0.01
        
        # è·å– warmup æ­¥æ•°
        warmup_steps = self.warmip_steps 

        # Warmup é˜¶æ®µ
        if update_count < warmup_steps:
            # é¿å…é™¤ä»¥ 0
            return max_lr

        # è¶…è¿‡æ€»æ­¥æ•°ä¿æŒæœ€å°å­¦ä¹ ç‡, é˜²æ­¢å´©æºƒ
        if update_count >= total_updates:
            return float(min_lr)

        progress = (update_count - warmup_steps) / (total_updates - warmup_steps)
        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
        current_lr = min_lr + (max_lr - min_lr) * cosine_decay
        
        return current_lr


MINI_CONFIG = Config(384, 6, 3, 9, 3, 4, 400000, 240000, 'gridman_mini')
SMALL_CONFIG = Config(512, 8, 4, 12, 4, 4, 400000, 300000, 'gridman_s')
MEDIUM_CONFIG = Config(768, 12, 6, 24, 4, 4, 1200000, 300000, 'gridman_m')
LAGRE_CONFIG = Config(1024, 16, 8, 36, 6, 4, 500000, 300000, 'gridman_l')

RUNNING_CONFIG = LAGRE_CONFIG
    

def preprocess_sft_dataset(dataset: DS | DatasetDict, config: Config, num_proc=20):
    block_byte_size = (config.chunk_size + 1) * config.patch_size 
    
    pad_token_id = config.tokenizer.pad_token_id
    eos_token_id = config.tokenizer.eos_token_id
    ignore_index = -100
    
    USER_PREFIX = "\nUser: "       
    ASST_PREFIX = "\nGridman: " 
    
    def format_and_mask(example):
        if 'conversations' in example:
            conversations = example['conversations']
        else:
            return {"input_ids": [], "labels": []}

        full_ids = []
        labels = []
        
        for msg in conversations:
            role = msg['role']
            content = msg['content']
            
            if role == 'user':
                text_chunk = USER_PREFIX + content
                ids = config.tokenizer.encode(text_chunk)
                full_ids.extend(ids)
                labels.extend([ignore_index] * len(ids))
                
            elif role == 'assistant':
                prefix_ids = config.tokenizer.encode(ASST_PREFIX)
                full_ids.extend(prefix_ids)
                labels.extend([ignore_index] * len(prefix_ids))
                
                content_ids = config.tokenizer.encode(content)
                full_ids.extend(content_ids)
                labels.extend(content_ids)
        
        full_ids.append(eos_token_id)
        labels.append(eos_token_id)
            
        return {
            "input_ids": full_ids,
            "labels": labels
        }

    # åŠ¨æ€æ£€æµ‹åˆ—å
    if isinstance(dataset, (dict, DatasetDict)):
        column_names = next(iter(dataset.values())).column_names
    else:
        column_names = dataset.column_names
    
    tokenized_datasets = dataset.map(
        format_and_mask, 
        num_proc=num_proc, 
        remove_columns=column_names
    )

    # åˆ†ç»„
    def group_texts(examples: dict):
        concatenated = {k: list(chain(*examples[k])) for k in ["input_ids", "labels"]}
        total_length = len(concatenated["input_ids"])
        
        remainder = total_length % block_byte_size
        if remainder != 0:
            pad_len = block_byte_size - remainder
            concatenated["input_ids"] += [pad_token_id] * pad_len
            concatenated["labels"] += [ignore_index] * pad_len
            total_length += pad_len
        
        result = {
            "input_ids": [concatenated["input_ids"][i : i + block_byte_size] 
                          for i in range(0, total_length, block_byte_size)],
            "labels":    [concatenated["labels"][i : i + block_byte_size] 
                          for i in range(0, total_length, block_byte_size)]
        }
        return result

    lm_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)
    return lm_datasets


def preprocess_and_group_bytes(dataset: DS | DatasetDict, config: Config, num_proc=20):
    # æ€»å­—èŠ‚é•¿åº¦ = (Patchæ•° + 1) * Patchå¤§å°
    block_byte_size = (config.chunk_size + 1) * config.patch_size 
    
    pad_token_id = config.tokenizer.pad_token_id
    
    def tokenize_function(examples):
        return {"input_ids": [config.tokenizer.encode(t.replace('<|im_end|>', '')) + [config.tokenizer.eos_token_id] for t in examples["text"]]}
    
    if isinstance(dataset, (dict, DatasetDict)):
        column_names = next(iter(dataset.values())).column_names
    else:
        column_names = dataset.column_names

    tokenized_datasets = dataset.map(
        tokenize_function, 
        batched=True, 
        remove_columns=column_names, 
        num_proc=num_proc
    )
    
    def group_texts(examples: dict):
        concatenated = {k: list(chain(*examples[k])) for k in examples.keys()}
        total_length = len(concatenated[list(examples.keys())[0]])
        
        # å¡«å…… (Padding)
        remainder = total_length % block_byte_size
        if remainder != 0:
            # è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦
            pad_len = block_byte_size - remainder
            # å¯¹ input_ids è¿›è¡Œå¡«å……
            concatenated["input_ids"] += [pad_token_id] * pad_len
            
            # æ›´æ–°æ€»é•¿åº¦
            total_length += pad_len
        
        result = {
            k: [t[i : i + block_byte_size] for i in range(0, total_length, block_byte_size)]
            for k, t in concatenated.items()
        }
        return result

    lm_datasets = tokenized_datasets.map(group_texts, batched=True, num_proc=num_proc)
    return lm_datasets


class BytePatchDataset(Dataset):
    def __init__(self, dataset: DS, patch_size: int):
        self.dataset = dataset
        self.patch_size = patch_size
        self.pad_id = 256  # å®šä¹‰ PAD ID

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        data = self.dataset[idx]
        
        full_input = torch.tensor(data['input_ids'], dtype=torch.long)
        num_patches = full_input.size(0) // self.patch_size
        
        reshaped_input = full_input[:num_patches*self.patch_size].view(num_patches, self.patch_size)
        input_patches = reshaped_input[:-1] 
        
        if 'labels' in data:
            # SFT 
            full_label = torch.tensor(data['labels'], dtype=torch.long)
            reshaped_label = full_label[:num_patches*self.patch_size].view(num_patches, self.patch_size)
            target_patches = reshaped_label[1:]
        else:
            # Pretrain 
            target_patches = reshaped_input[1:].clone()
            target_patches[target_patches == self.pad_id] = -100
        
        return input_patches, target_patches


class OuroDataLoader:
    """
    æ•°æ®åŠ è½½å™¨
    """
    def __init__(self, raw_datasets: DS | DatasetDict, config: Config, 
                 split: str = 'train', global_step: int = 0, is_sft: bool = False,
                 num_workers=20, num_proc=20
                 ):
        
        self.config = config
        self.is_train = (split == 'train')
        
        print(f"ğŸ“¦ Initializing OuroDataLoader for [{split}] (SFT={is_sft})...")

        # é¢„å¤„ç†ç­–ç•¥è·¯ç”±
        if is_sft:
            processed_dict = preprocess_sft_dataset(raw_datasets, config, num_proc)
        else:
            processed_dict = preprocess_and_group_bytes(raw_datasets, config, num_proc)
        
        hf_dataset = processed_dict[split]
        total_len = len(hf_dataset)
        
        # ä»…é’ˆå¯¹è®­ç»ƒé›†å¿«è¿›
        if self.is_train and global_step > 0:
            # èŠ‚å¾‹è®¡ç®—
            num_cycles = global_step // config.cycle_len
            remainder = global_step % config.cycle_len
            
            # è®¡ç®—å®é™…æ¶ˆè€—çš„ Batch æ•°
            consumed_batches = num_cycles * config.wake_steps + min(remainder, config.wake_steps)
            
            # å¤„ç† Epoch å¾ªç¯
            start_idx = consumed_batches % total_len
            
            print(f"â© Fast-forwarding: Step {global_step} => Consumed {consumed_batches} batches.")
            print(f"âœ‚ï¸  Slicing dataset from index {start_idx} to {total_len} (Skipped {consumed_batches} items total).")
            
            # åˆ‡ç‰‡
            hf_dataset = hf_dataset.select(range(start_idx, total_len))
        
        self.patch_dataset = BytePatchDataset(hf_dataset, config.patch_size)
        
        self.loader = DataLoader(
            self.patch_dataset,
            batch_size=1,
            shuffle=False, 
            num_workers=num_workers,
            pin_memory=self.is_train
        )

    def __iter__(self):
        return iter(self.loader)

    def __len__(self):
        return len(self.loader)
    

def states_clone(states: Tuple[Optional[torch.Tensor], ...], need_clone=True):
    res = []
    for state in states:
        if state is not None:
            state: torch.Tensor
            if need_clone:
                res.append(state.clone().detach())
            else:
                res.append(state.detach())
        else:
            res.append(None)
    return tuple(res)
    

def save_checkpoint(
    step: int, 
    update_count: int, 
    model: nn.Module, 
    states: tuple[torch.Tensor, ...], 
    path: str,
    override_model_dict=None
):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    # å°† Tuple çŠ¶æ€è½¬ä¸º CPU
    states_cpu = tuple(s.detach().cpu() for s in states)
    
    # ä¿®æ­£åçš„æƒé‡å­—å…¸, å¦åˆ™è·å–å½“å‰æ¨¡å‹çš„
    model_dict = override_model_dict if override_model_dict is not None else model.state_dict()
    
    checkpoint_dict = {
        'step': step,
        'update_count': update_count,
        'model': model_dict,
        'states': states_cpu,
        # ä¿å­˜éšæœºæ•°çŠ¶æ€
        'rng_state': torch.get_rng_state(),
        'cuda_rng_state': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None
    }

    torch.save(checkpoint_dict, path)
    print(f'ğŸ’¾ Saved checkpoint to {path} (Step {step})')


def load_checkpoint(
    path: str, 
    model: nn.Module, 
    optimizer: torch.optim.AdamW, 
    device: torch.device,
    lr: Callable[[int, bool], float]
):
    if not os.path.exists(path): 
         # å°† target_lr åº”ç”¨åˆ°ä¼˜åŒ–å™¨
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr(0)
            param_group['initial_lr'] = lr(0)
        return 0, 0, None
        
    print(f"ğŸ“‚ Loading checkpoint from {path}")
    ckpt: dict = torch.load(path, map_location=device)
    
    # åŠ è½½æ¨¡å‹
    model.load_state_dict(ckpt['model'])

    # è·å–æ­¥æ•°, æ›´æ–°æ¬¡æ•°
    steps: int = ckpt['step']
    update_count: int = ckpt['update_count']

    # å°† target_lr åº”ç”¨åˆ°æ–°ä¼˜åŒ–å™¨
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr(update_count)
        param_group['initial_lr'] = lr(update_count)
    
    # æ¢å¤éšæœºçŠ¶æ€
    if 'rng_state' in ckpt:
        torch.set_rng_state(ckpt['rng_state'].cpu().byte())
    if 'cuda_rng_state' in ckpt and torch.cuda.is_available():
        try:
            torch.cuda.set_rng_state_all([s.cpu().byte() for s in ckpt['cuda_rng_state']])
        except: pass

    _states = tuple(s.to(device) for s in ckpt['states'])
    return steps, update_count, _states